\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Zero-Shot Sketch-Based Image Retrieval with Cross-modal Translation}

\author{Jiangtong Li, Yi Xu$^{\dag}$ \thanks{$^{\dag}$ Corresponding author. If you have any question, you can contact with \tt{xuyi\_2019@sjtu.edu.cn}},  Yi'an Li, Siqi Liu, Jiarui Jin  \\
Department of Computer Science and Engineering, Shanghai Jiao Tong University \\
}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both



\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   Zero-shot sketch-based image retrieval (ZS-SBIR) is a task of cross-domain image retrieval from a natural image gallery with free-hand sketch under a zero-shot scenario. 
   Previous works mostly focus on a generative approach that takes a highly abstract and sparse sketch as input and then synthesizes the corresponding natural image. 
   However, fusing the sketch features and image features together during encoding weakens the generative capability of the decoder, besides, the generative models prone to use paired data to during training, which also waste a large amount of unpaired data and hence achieve unsatisfactory retrieval performance. 
   In this paper, we propose a novel cross-modal domain translation network to make use of the large amount of unpaired data and strengthen the decoder. 
   Specifically, we devise an asymmetrical translation network, which divides the hidden space into structure space and appearance space. To compensate uncertain appearance information in structure space, we also add an extra information compensation module before image decoding.
   In order to make use of the large amount unpaired data, we adopt a ranking loss in structure space and design a new retrieval criterion to take advantage from both regressive model and generative model.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

\textbf{The importance of our research area} \\

\textbf{Some progress in sketch based image retrieval} \\

\textbf{Difficulty in Zero-shot setup and some possible solutions} \\

\textbf{Our proposed methods and their advantages} \\

\textbf{itemize our contributions in this paper} \\




%\begin{figure}[t]
%\begin{center}
%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
%\end{center}
%   \caption{Example of caption.  It is set in Roman so that mathematics
%   (always set in Roman: $B \sin A = A \sin B$) may be included without an
%   ugly clash.}
%\label{fig:long}
%\label{fig:onecol}
%\end{figure}




%\begin{figure*}
%\begin{center}
%\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%\end{center}
%   \caption{Example of a short caption, which should be centered.}
%\label{fig:short}
%\end{figure*}

\section{Related Work}

\subsection{Sketch-based image retrieval}

\subsection{Zero-Shot Learning}

\subsection{Disentangled Representation}

\section{Methodology}
%There will be five parts in this section. Sec. \ref{3.1} defines the our targeted problem and briefly introduce our framework. Sec.\ref{3.2} introduce the encoders in our model. Sec. \ref{3.3} introduce the decoder in our model. Sec. \ref{3.4} introduce the discriminator in our model. Sec. \ref{3.5} introduce the design of loss functions during training procedure.

\subsection{Problem Definition and Overall model}\label{3.1}
%In this paper, we focus on solving the problem of hand-free sketch-based image retrieval using disentangled feature representation under zero-shot setup, where only the sketchs and images from seen classes are used during training stage. Our proposed framework is expected to use the sketchs to retrieve the images, the categories of which have never appeared during training.

%We first provide a definition of the SBIR in zero-shot setup. Given a dataset $S=\{(x_i^{img}, x_i^{ske}, x_i^{sem}, y_i)|y_i \in \mathcal{Y}\}$, where $x_i^{img}$, $x_i^{ske}$, $x_i^{sem}$ and $y_i$ corresponding to the image, sketch, semantic representation and 

\subsection{Encoder}\label{3.2}

\subsection{Generator}\label{3.3}

\subsection{Discriminator}\label{3.4}

\subsection{Loss Function}\label{3.5}



%\begin{table}
%\begin{center}
%\begin{tabular}{|l|c|}
%\hline
%Method & Frobnability \\
%\hline\hline
%Theirs & Frumpy \\
%Yours & Frobbly \\
%Ours & Makes one's heart Frob\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Results.   Ours is better.}
%\end{table}

\section{Experiment}

\subsection{Experiment Setup}

\subsubsection{Dataset}

\subsubsection{Implementation Details}

\subsection{Comparison}

\subsection{Ablation Study}

\subsection{Case study}

\section{Conclusion}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
