\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{\LaTeX\ Author Guidelines for CVPR Proceedings}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   pass
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

\textbf{The importance of our research area} \\

\textbf{Some progress in sketch based image retrieval} \\

\textbf{Difficulty in Zero-shot setup and some possible solutions} \\

\textbf{Our proposed methods and their advantages} \\

\textbf{itemize our contributions in this paper} \\




%\begin{figure}[t]
%\begin{center}
%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
%\end{center}
%   \caption{Example of caption.  It is set in Roman so that mathematics
%   (always set in Roman: $B \sin A = A \sin B$) may be included without an
%   ugly clash.}
%\label{fig:long}
%\label{fig:onecol}
%\end{figure}




%\begin{figure*}
%\begin{center}
%\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%\end{center}
%   \caption{Example of a short caption, which should be centered.}
%\label{fig:short}
%\end{figure*}

\section{Related Work}

\subsection{Sketch-based image retrieval}

\subsection{Zero-Shot Learning}

\subsection{Cross-modal domain translation}

\section{Methodology}
There will be five parts in this section. Sec. \ref{3.1} defines the our targeted problem and briefly introduce our framework. Sec.\ref{3.2} introduce the encoders in our model. Sec. \ref{3.3} introduce the decoder in our model. Sec. \ref{3.4} introduce the discriminator in our model. Sec. \ref{3.5} introduce the design of loss functions during training procedure.

\subsection{Problem Definition} \label{3.1}
In this paper, we focus on solving the problem of hand-free sketch-based image retrieval under zero-shot setup, where only the sketches and images from seen class are used during training stage. Our proposed framework is expected to use the sketchs to retrieve the images, the categories of which have never appeared during training.

We first provide a definition of the SBIR in zero-shot setting. Given a dataset $S=\{(x_i^{img}, x_i^{ske}, x_i^{sem}, y_i)|y_i \in \mathcal{Y}\}$, where $x_i^{img}$, $x_i^{ske}$, $x_i^{sem}$ and $y_i$ are corresponding to the image, sketch, semantic representation and class label. Following the zero-shot setting in \cite{yelamarthi2018zero}, we split all classes $\mathcal{Y}$ into $\mathcal{Y}_{train}$ and $\mathcal{Y}_{test}$ according to whether the label exists in ImageNet\cite{deng2009imagenet}, where no overlap exists between two label set, i.e. $\mathcal{Y}_{train} \cap \mathcal{Y}_{test} = \emptyset$. Based on the partition of label set $\mathcal{Y}$, we split dataset into $S_{train}$ and $S_{test}$. Our model need to disentangle structure representations of image using data in $S_{train}$. During test, given $x^{ske}$ from $S_{test}$, our model need to retrieve several images from test images candidate.

Our goal is to learn a two-way map between image feature domain to sketch feature domain. To this end, we propose a new deep network (shown in Figure \ref{fig:2}), which contains two structure encoders $\{E_{s}^{img}, E^{ske}\}$, one apearance encoder $E^{img}_{a}$, two feature decoder $\{G^{img}, G^{ske}\}$, a semantic decoder and two domain discriminators $\{D^{img}, D^{ske}\}$. Note that, the overall model can be regrad as two cVAE-GANs working parallelly, which target to reconstruct sketch features from image and reconstruct image features from both sketches and images. To better capture the semantics information inside the sketches and images, we also add a semantic decoder to preserve semantics information while reconstructing the image features.

%Compare with the image, the sketch are obvious lack of image information, which makes it hard to reconstruct image features from sketch only

\subsection{Encoder} \label{3.2}

\subsection{Generator} \label{3.3}

\subsection{Discriminator} \label{3.4}

\subsection{Loss Function} \label{3.5}



%\begin{table}
%\begin{center}
%\begin{tabular}{|l|c|}
%\hline
%Method & Frobnability \\
%\hline\hline
%Theirs & Frumpy \\
%Yours & Frobbly \\
%Ours & Makes one's heart Frob\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Results.   Ours is better.}
%\end{table}

\section{Experiment}

\subsection{Experiment Setup}

\subsubsection{Dataset}

\subsubsection{Implementation Details}

\subsection{Comparison}

\subsection{Ablation Study}

\subsection{Case study}

\section{Conclusion}

\section{To Discuss}
\begin{itemize}
	\item Whether to generator the whole image/sketch.
	\item If the poses between the image and the sketch are different, can the model learn the sketch information between image and sketch.
	\item Where to add the semantics information to further supervise the model's training.
\end{itemize}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
