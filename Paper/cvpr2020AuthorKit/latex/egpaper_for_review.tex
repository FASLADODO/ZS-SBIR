\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{\LaTeX\ Author Guidelines for CVPR Proceedings}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   pass
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

\textbf{The importance of our research area} \\

\textbf{Some progress in sketch based image retrieval} \\

\textbf{Difficulty in Zero-shot setup and some possible solutions} \\

\textbf{Our proposed methods and their advantages} \\

\textbf{itemize our contributions in this paper} \\




%\begin{figure}[t]
%\begin{center}
%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
%\end{center}
%   \caption{Example of caption.  It is set in Roman so that mathematics
%   (always set in Roman: $B \sin A = A \sin B$) may be included without an
%   ugly clash.}
%\label{fig:long}
%\label{fig:onecol}
%\end{figure}




%\begin{figure*}
%\begin{center}
%\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%\end{center}
%   \caption{Example of a short caption, which should be centered.}
%\label{fig:short}
%\end{figure*}

\section{Related Work}

\subsection{Sketch-based image retrieval}

\subsection{Zero-Shot Learning}

\subsection{Cross-modal domain translation}

\section{Methodology}
There will be five parts in this section. Sec. \ref{3.1} defines the our targeted problem and briefly introduce our framework. Sec.\ref{3.2} introduce the feature extractor. Sec. \ref{3.3} introduce the parallel cVAE-GANs. Sec. \ref{3.4} introduce the semantic preservation module. Sec. \ref{3.5} introduce the design of loss functions during training procedure.

\subsection{Problem Definition} \label{3.1}
In this paper, we focus on solving the problem of hand-free sketch-based image retrieval under zero-shot setup, where only the sketches and images from seen class are used during training stage. Our proposed framework is expected to use the sketches to retrieve the images, the categories of which have never appeared during training.

We first provide a definition of the SBIR in zero-shot setting. Given a dataset $S=\{(x_i^{img}, x_i^{ske}, x_i^{sem}, y_i)|y_i \in \mathcal{Y}\}$, where $x_i^{img}$, $x_i^{ske}$, $x_i^{sem}$ and $y_i$ are corresponding to the image, sketch, semantic representation and class label. Following the zero-shot setting in \cite{yelamarthi2018zero}, we split all classes $\mathcal{Y}$ into $\mathcal{Y}_{train}$ and $\mathcal{Y}_{test}$ according to whether the label exists in ImageNet\cite{deng2009imagenet}, where no overlap exists between two label set, i.e. $\mathcal{Y}_{train} \cap \mathcal{Y}_{test} = \emptyset$. Based on the partition of label set $\mathcal{Y}$, we split dataset into $S_{train}$ and $S_{test}$. Our model need to disentangle structure representations of image using data in $S_{train}$. During test, given $x^{ske}$ from $S_{test}$, our model need to retrieve several images from test images candidate.

Our goal is to learn a two-way map between image feature domain to sketch feature domain. To this end, we propose a new deep network (shown in Figure \ref{fig:2}), which contains feature extractor $F(\cdot)$, two structure encoders $\{E_{s}^{img}(\cdot), E^{ske}(\cdot)\}$, one appearance encoder $E^{img}_{a}(\cdot)$, two feature decoder $\{G^{img}(\cdot, \cdot), G^{ske}(\cdot)\}$, a semantic decoder $S(\cdot)$ and two domain discriminators $\{D^{img}(\cdot, \cdot), D^{ske}(\cdot, \cdot)\}$. Note that by combining the encoders, decoders and discriminators, our model can be regarded as two cVAE-GANs working parallel, which target to reconstruct sketch features from image and reconstruct image features from both sketches and images. To better preserve the semantics information within the sketches and images, we also add a semantic decoder to preserve semantics information while reconstructing the image features.

%Compare with the image, the sketch are obvious lack of image information, which makes it hard to reconstruct image features from sketch only

\subsection{Feature Extractor} \label{3.2}
Considering the abstractness and visual sparsity of sketch, it is challenging extract feature from sketch. To alleviate this issue, multi-channel and multi-scale model was proposed to extract more saint features \cite{yu2017sketch}. Motivated by the visualization in \cite{zeiler2014visualizing}, where different layers in the backbone model capture visual features at different levels, we build our feature extractor by combining different layers' features together to enrich feature representation capacity without adding any additional parameters.

In detail, we first use a pre-trained backbone model on ImageNet \cite{deng2009imagenet} to process each sketch and image, where we use VGG-16 as our backbone model in this paper. Suppose $f_i \in \mathbb{R}^{C_i \times H_i \times W_i}$ is the output of the $i$-th convolution module and $f_{fc} \in \mathbb{R}^{N}$ is the feature of the last fully connected layer, the extracted feature $f^{\cdot}$ can be formulated as:

\begin{align}
f^{\cdot} = F(x_i^{\cdot}) = [f_{fc}, GAP(f_5), GAP(f_4), GAP(f_3)]
\end{align}
where $GAP(\cdot)$ means global spatial average pooling and $[\cdot,\cdot]$ means the concatenation operation between vectors. Following the instruction in \cite{wang2019stacked}, a Principal Component Analysis algorithm is adopted to reduce the dimension of the extracted feature, which is helpful to not only improve the computational effiency but remove some redundant information.

\subsection{Parallel cVAE-GAN} \label{3.3}
Generative approaches have shown their power in ZS-SBIR \cite{wang2019stacked,yelamarthi2018zero}. But both of these two papers, only encode the image in one encoder, which makes the appearance and the structure features fused together. When training the VAE based model, the decoder may ignore the condition information in sketch feature. Motivated by \cite{zhu2017toward}, we can regard image and sketch as two domains, where image domain have richer information than sketch domain. To this end, we use three encoders to encode image and sketch features respectively. Given image and sketch 

\subsection{Semantics Preservation} \label{3.4}

\subsection{Loss Function} \label{3.5}



%\begin{table}
%\begin{center}
%\begin{tabular}{|l|c|}
%\hline
%Method & Frobnability \\
%\hline\hline
%Theirs & Frumpy \\
%Yours & Frobbly \\
%Ours & Makes one's heart Frob\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Results.   Ours is better.}
%\end{table}

\section{Experiment}

\subsection{Experiment Setup}

\subsubsection{Dataset}

\subsubsection{Implementation Details}

\subsection{Comparison}

\subsection{Ablation Study}

\subsection{Case study}

\section{Conclusion}

\section{To Discuss}
\begin{itemize}
	\item Whether to generator the whole image/sketch.
	\item If the poses between the image and the sketch are different, can the model learn the sketch information between image and sketch.
	\item Where to add the semantics information to further supervise the model's training.
\end{itemize}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
